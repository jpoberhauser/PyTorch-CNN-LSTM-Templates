{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U_Net_Pytorch_Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKKKTzWPk9yo5YXTRTTGxJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFdK6tsAxNKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfTFWsLTxXfs",
        "colab_type": "text"
      },
      "source": [
        "U-NEts are used for classification and segmentation.\n",
        "\n",
        "* Need to know: What are convolutions, strides, padding, max pooling, transposed convolution, ReLU. \n",
        "\n",
        "* Single channel image 572x572 then you do a convolution and the image sixe reduces (there is no padding). Repeat this pattern.\n",
        "\n",
        "* Double_conv function should do two convs, then max pools, then another 2 convolutions.. repeat that. \n",
        "\n",
        "* At the middle we start to apply up-convolutions. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Lookup\n",
        "\n",
        "+ bi-linear upsampling (lookup)\n",
        "\n",
        "+ up-convolutions (lookup)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8ivH0Gzs5O",
        "colab_type": "text"
      },
      "source": [
        "## More on the architecture:\n",
        "\n",
        "It consistes of the repeated application of two 3x3 convolutions (unpadded convolutions) each followed by a rectifued linear unit, and a 2x2 max pooling opertaion with stride = 2 for downsampling. \n",
        "\n",
        "\n",
        "Transpose convolutions will increase the size. Convolutions without padding decrease the size. \n",
        "\n",
        "You can use ConvTransposed2d from PyTorch: From an image of size 28x28 we need to get an image of size 56. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bepTxOaOyqoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Unet.py\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHAr6Vmu0lNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def double_conv(in_c, out_c):\n",
        "  #Make a helper function that performs a double convolution with Relu\n",
        "  conv = nn.Sequential(\n",
        "     nn.Conv2d(in_c, out_c, kernel_size = 3),\n",
        "     nn.ReLU(inplace = True),\n",
        "     nn.Conv2d(out_c,out_c, kernel_size = 3),\n",
        "     nn.ReLU(inplace = True)\n",
        "     )\n",
        "  return conv\n",
        "\n",
        "\n",
        "\n",
        "def crop_image(orig_tensor, target_sensor):\n",
        "  target_size = target_sensor.size()[2]\n",
        "  orig_tensor_size = orig_tensor.size()[2]\n",
        "  delta = orig_tensor_size - target_size\n",
        "  delta = delta // 2\n",
        "  return orig_tensor[:,:, delta:orig_tensor_size-delta, delta:orig_tensor_size-delta]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO79Ul4Ry77Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        # first thing we have is a input of 572x572 with 2 convolutions\n",
        "        # one channel image is converted to 64 channels and then another 64 channels.\n",
        "        super(UNet, self).__init__()\n",
        "        # we need max pooling\n",
        "        self.max_pool_2x2 = nn.MaxPool2d(kernel_size = 2, stride = 2) #2x2 max pooling with stride of 2\n",
        "        # we also need the double convolution\n",
        "        self.down_conv1 = double_conv(1, 64)\n",
        "        self.down_conv2 = double_conv(64, 128)\n",
        "        self.down_conv3 = double_conv(128, 256)\n",
        "        self.down_conv4 = double_conv(256, 512)\n",
        "        self.down_conv5 = double_conv(512, 1024)\n",
        "\n",
        "        self.up_trans_1 = nn.ConvTranspose2d(\n",
        "                                  in_channels = 1024, \n",
        "                                  out_channels = 512, \n",
        "                                  kernel_size = 2,\n",
        "                                  stride = 2)# need to increase the size by two. \n",
        "\n",
        "        self.up_conv_1 =  double_conv(1024, 512)# the output channels were 512, but you combined it with the concatenation so it turns into 1024\n",
        "        self.up_trans_2 = nn.ConvTranspose2d(\n",
        "                                  in_channels = 512, \n",
        "                                  out_channels = 256, \n",
        "                                  kernel_size = 2,\n",
        "                                  stride = 2)# need to increase the size by two. \n",
        "\n",
        "        self.up_conv_2 =  double_conv(512, 256)\n",
        "\n",
        "        self.up_trans_3 = nn.ConvTranspose2d(\n",
        "                                  in_channels = 256, \n",
        "                                  out_channels = 128, \n",
        "                                  kernel_size = 2,\n",
        "                                  stride = 2)# need to increase the size by two. \n",
        "\n",
        "        self.up_conv_3 =  double_conv(256, 128)\n",
        "\n",
        "        self.up_trans_4 = nn.ConvTranspose2d(\n",
        "                                  in_channels = 128, \n",
        "                                  out_channels = 64, \n",
        "                                  kernel_size = 2,\n",
        "                                  stride = 2)# need to increase the size by two. \n",
        "\n",
        "        self.up_conv_4 =  double_conv(128, 64)\n",
        "\n",
        "\n",
        "        self.out =  nn.Conv2d(in_channels = 64, out_channels = 2,#out channels here is two since it is two class\n",
        "                              kernel_size = 1)\n",
        "        \n",
        " \n",
        "    \n",
        "    def forward(self, image):\n",
        "        #encoder\n",
        "        #bs, c, h, w\n",
        "        x1 = self.down_conv1(image)# we need to pass this to the last up-conv layer\n",
        "        print(\"X1 after 1st conv\", x1.size())\n",
        "        x1_pooled = self.max_pool_2x2(x1) \n",
        "        x2 = self.down_conv2(x1_pooled) # this need to pass to the second to last up-conv layer. \n",
        "        x2_pooled = self.max_pool_2x2(x2)\n",
        "        x3 = self.down_conv3(x2_pooled) # this goes to third to last\n",
        "        x3_pooled = self.max_pool_2x2(x3)\n",
        "        x4 = self.down_conv4(x3_pooled) # this goes to fourth to last\n",
        "        print(\"x4 after conv\", x4.size())\n",
        "        x4_pooled = self.max_pool_2x2(x4)\n",
        "        x5 = self.down_conv5(x4_pooled)\n",
        "        print(\"X5 after last down convolution\", x5.size())\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        x = self.up_trans_1(x5) # we need to concatenate x4 [1, 512, 64, 64] to x which is [1, 512, 56, 56]\n",
        "        y = crop_image(x4, x)\n",
        "        x = self.up_conv_1(torch.cat([x,y], 1))\n",
        "        print(\"result of up_conv\", x.shape)\n",
        "        x = self.up_trans_2(x) # we need to concatenate x4 [1, 512, 64, 64] to x which is [1, 512, 56, 56]\n",
        "        y = crop_image(x3, x)\n",
        "        x = self.up_conv_2(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.up_trans_3(x) # we need to concatenate x4 [1, 512, 64, 64] to x which is [1, 512, 56, 56]\n",
        "        y = crop_image(x2, x)\n",
        "        x = self.up_conv_3(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.up_trans_4(x) # we need to concatenate x4 [1, 512, 64, 64] to x which is [1, 512, 56, 56]\n",
        "        y = crop_image(x1, x)\n",
        "        x = self.up_conv_4(torch.cat([x,y], 1))\n",
        "\n",
        "        x = self.out(x)\n",
        "        print(\"results\", x.shape)\n",
        "        return x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJgro3X-3FgY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "outputId": "3278efb1-2b98-406a-dc70-790efa36f5f2"
      },
      "source": [
        "# if __name__ == \"__main__\":\n",
        "\n",
        "image = torch.rand(1, 1, 572, 572)\n",
        "model = UNet()\n",
        "model(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 after 1st conv torch.Size([1, 64, 568, 568])\n",
            "x4 after conv torch.Size([1, 512, 64, 64])\n",
            "X5 after last down convolution torch.Size([1, 1024, 28, 28])\n",
            "result of up_conv torch.Size([1, 512, 52, 52])\n",
            "results torch.Size([1, 2, 388, 388])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 1.9717e-03,  2.8767e-03,  4.3008e-03,  ...,  3.7842e-03,\n",
              "            2.4997e-03,  2.9996e-03],\n",
              "          [ 2.7762e-03,  3.3202e-03,  2.5759e-03,  ...,  3.1935e-03,\n",
              "            9.7582e-05,  3.6280e-03],\n",
              "          [ 6.1720e-03,  3.6901e-03,  7.1466e-03,  ...,  2.0450e-03,\n",
              "            2.9575e-03,  2.7717e-03],\n",
              "          ...,\n",
              "          [ 2.5501e-03,  2.6572e-03,  3.0521e-03,  ...,  3.3637e-03,\n",
              "            7.6788e-04,  4.2437e-03],\n",
              "          [ 5.7757e-04,  2.2286e-04,  2.8272e-03,  ...,  5.5916e-03,\n",
              "            3.1097e-03,  4.5494e-03],\n",
              "          [ 1.6284e-03,  3.2798e-03,  5.1131e-03,  ...,  2.2878e-03,\n",
              "           -2.9861e-04,  2.3731e-03]],\n",
              "\n",
              "         [[-1.0958e-01, -1.1289e-01, -1.1193e-01,  ..., -1.0959e-01,\n",
              "           -1.0844e-01, -1.1241e-01],\n",
              "          [-1.0923e-01, -1.0782e-01, -1.1300e-01,  ..., -1.0924e-01,\n",
              "           -1.0926e-01, -1.0921e-01],\n",
              "          [-1.1016e-01, -1.0771e-01, -1.1001e-01,  ..., -1.1016e-01,\n",
              "           -1.0844e-01, -1.0752e-01],\n",
              "          ...,\n",
              "          [-1.1186e-01, -1.0808e-01, -1.1165e-01,  ..., -1.1015e-01,\n",
              "           -1.0773e-01, -1.1177e-01],\n",
              "          [-1.0920e-01, -1.1087e-01, -1.1003e-01,  ..., -1.0860e-01,\n",
              "           -1.0704e-01, -1.0875e-01],\n",
              "          [-1.1190e-01, -1.0795e-01, -1.0929e-01,  ..., -1.1024e-01,\n",
              "           -1.0778e-01, -1.0801e-01]]]], grad_fn=<MkldnnConvolutionBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4ImHR9d3vLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}